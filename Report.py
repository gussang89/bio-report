import streamlit as st
import urllib.request
import urllib.parse
import json
import re
import google.generativeai as genai
from datetime import datetime, timedelta
import io
from docx import Document
import xml.etree.ElementTree as ET
from email.utils import parsedate_to_datetime

# --- 1. êµ¬ê¸€ ì œë¯¸ë‚˜ì´ ì„¤ì • ---
def configure_gemini():
    if "GOOGLE_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
        return True
    return False

# --- 2. ê²€ìƒ‰ í•¨ìˆ˜ë“¤ ---

# [1] Europe PMC (í•´ì™¸ ë…¼ë¬¸)
def get_epmc_papers(keywords, months):
    query_parts = [f'({k.strip()})' for k in keywords if k.strip()]
    if not query_parts: return []
    keyword_query = " OR ".join(query_parts)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=months*30)
    date_query = f'FIRST_PDATE:[{start_date.strftime("%Y-%m-%d")} TO {end_date.strftime("%Y-%m-%d")}]'
    full_query = f"({keyword_query}) AND ({date_query})"
    encoded_query = urllib.parse.quote(full_query)
    base_url = f"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query={encoded_query}&format=json&resultType=core&pageSize=30"
    
    try:
        req = urllib.request.Request(base_url, headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req) as response:
            data = json.loads(response.read().decode('utf-8'))
        filtered_papers = []
        for p in data.get('resultList', {}).get('result', []):
            title = p.get('title', '')
            abstract = re.sub('<[^<]+>', '', p.get('abstractText', ''))
            doi = p.get('doi')
            link = f"https://doi.org/{doi}" if doi else ""
            if title and abstract:
                filtered_papers.append({"title": title, "abstract": abstract, "url": link, "date": p.get('firstPublicationDate', '')})
        return filtered_papers
    except Exception as e:
        return []

# [2] Google News RSS (êµ­ë‚´ì™¸ ë‰´ìŠ¤ - API í‚¤ í•„ìš” ì—†ìŒ!)
def get_google_news(keywords, months):
    query_parts = [f'"{k.strip()}"' for k in keywords if k.strip()]
    if not query_parts: return []
    search_query = " OR ".join(query_parts)
    encoded_query = urllib.parse.quote(search_query)
    
    # í•œêµ­(ko) ë° ë¯¸êµ­(en-US) ë‰´ìŠ¤ ë™ì‹œ ê²€ìƒ‰
    urls = [
        f"https://news.google.com/rss/search?q={encoded_query}+when:{months}m&hl=ko&gl=KR&ceid=KR:ko",
        f"https://news.google.com/rss/search?q={encoded_query}+when:{months}m&hl=en-US&gl=US&ceid=US:en"
    ]
    
    news_list = []
    for url in urls:
        source_label = "ğŸ‡°ğŸ‡· êµ­ë‚´" if "hl=ko" in url else "ğŸŒ í•´ì™¸"
        try:
            req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
            with urllib.request.urlopen(req) as response:
                xml_data = response.read()
            root = ET.fromstring(xml_data)
            
            for item in root.findall('./channel/item'):
                title = item.find('title').text
                link = item.find('link').text
                pubDate = item.find('pubDate').text
                
                # ë‚ ì§œ ë³€í™˜
                try:
                    dt = parsedate_to_datetime(pubDate)
                    date_str = dt.strftime("%Y-%m-%d")
                except:
                    date_str = pubDate
                
                news_list.append({
                    "title": title, 
                    "abstract": "ìƒì„¸ ë‚´ìš©ì€ ì›ë¬¸ ë§í¬ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.", # RSSëŠ” ìš”ì•½ì´ ë§¤ìš° ì§§ì•„ ì œëª© ìœ„ì£¼ë¡œ í™œìš©
                    "url": link, 
                    "date": date_str,
                    "source": source_label
                })
        except Exception as e:
            continue
            
    # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€) ë° ìµœì‹ ìˆœ ì •ë ¬
    unique_news = {n['url']: n for n in news_list}.values()
    sorted_news = sorted(unique_news, key=lambda x: x['date'], reverse=True)
    return list(sorted_news)

# --- 3. AI ë¦¬í¬íŠ¸ ìƒì„± (í†µí•© í•¨ìˆ˜) ---
def generate_ai_report(items, keywords, context_type):
    if not items: return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    data_text = ""
    for i, item in enumerate(items[:30]): # ë‰´ìŠ¤ ì œëª©ì´ ì§§ìœ¼ë¯€ë¡œ 30ê°œê¹Œì§€ ë¶„ì„
        prefix = f"[{item.get('source', '')}] " if 'source' in item else ""
        data_text += f"[{i+1}] {prefix}ì œëª©: {item['title']} (ì¼ì: {item['date']})\nì´ˆë¡: {item['abstract'][:200]}\n\n"

    if context_type == "Global_Papers":
        role_description = "ê¸€ë¡œë²Œ ë°”ì´ì˜¤ ì—ë„ˆì§€ ê³µì • ì—”ì§€ë‹ˆì–´"
        focus_point = """
        1. ğŸ”¬ **ê¸°ìˆ  íŠ¸ë Œë“œ ìš”ì•½**: í•µì‹¬ ê³µì • ë° ìµœì‹  ê¸°ìˆ  ë™í–¥
        2. ğŸ­ **ê³µì • ìµœì í™” ì¸ì‚¬ì´íŠ¸**: ìˆ˜ìœ¨ ê°œì„  ë° ìœ í‹¸ë¦¬í‹° ì ˆê° ì‹œì‚¬ì 
        3. ğŸ† **ì£¼ìš” ë…¼ë¬¸ 3ì„ **: ëˆˆì—¬ê²¨ë³¼ í•µì‹¬ ë…¼ë¬¸ ìš”ì•½ (ê° í•­ëª© ëì— ì£¼ì„ í˜•íƒœë¡œ ì›ë¬¸ ë§í¬ ë²ˆí˜¸ í‘œê¸°)
        """
    else: # News
        role_description = "ë°”ì´ì˜¤ ì—ë„ˆì§€ ì‚°ì—… ë° ì‹œì¥ ì• ë„ë¦¬ìŠ¤íŠ¸"
        focus_point = """
        1. ğŸ“° **ì‹œì¥ ë° ì‚°ì—… ë™í–¥**: ê¸€ë¡œë²Œ ë° êµ­ë‚´ ë°”ì´ì˜¤ ì—°ë£Œ(SAF, HVO ë“±) ì‹œì¥ì˜ ê±°ì‹œì  íë¦„
        2. ğŸ›ï¸ **ì •ì±… ë° íˆ¬ì ë™í–¥**: ê°êµ­ ì •ë¶€ì˜ ê·œì œ ë³€í™”ë‚˜ ì£¼ìš” ê¸°ì—…ì˜ íˆ¬ì/ìƒìš©í™” ë°œí‘œ
        3. ğŸ’¡ **ì‹œì‚¬ì **: í˜„ì—…ì—ì„œ ì£¼ëª©í•´ì•¼ í•  ë¦¬ìŠ¤í¬ ë° ê¸°íšŒ ìš”ì¸ (ê° í•­ëª© ëì— ì£¼ì„ í˜•íƒœë¡œ ë‰´ìŠ¤ ì›ë¬¸ ë²ˆí˜¸ í‘œê¸°)
        """

    prompt = f"""
    ë‹¹ì‹ ì€ {role_description}ì…ë‹ˆë‹¤. í‚¤ì›Œë“œ: {', '.join(keywords)}
    
    ì•„ë˜ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ **'ì‹¬ì¸µ ë¦¬í¬íŠ¸'**ë¥¼ A4 1~2í˜ì´ì§€ ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    *ì£¼ì˜ì‚¬í•­: AIì˜ ì¶”ë¡ ì´ ë“¤ì–´ê°„ ë¶€ë¶„ì€ 'ì¶”ì •' ë˜ëŠ” 'ì˜ˆìƒ'ì„ì„ ëª…í™•íˆ ë°íˆê³ , ê¸°ì¬ëœ ì‚¬ì‹¤ì€ ì œê³µëœ ë°ì´í„°(ì£¼ì„ ë²ˆí˜¸)ë¥¼ ê·¼ê±°ë¡œ ì‘ì„±í•˜ì„¸ìš”.

    [ì‘ì„± í¬ì¸íŠ¸]
    {focus_point}

    [ìˆ˜ì§‘ëœ ë°ì´í„°]
    {data_text}
    """
    
    try:
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        preferred_order = ['models/gemini-1.5-pro', 'models/gemini-1.5-flash', 'models/gemini-pro']
        sorted_models = [m for m in preferred_order if m in available_models] + [m for m in available_models if m not in preferred_order]

        for model_name in sorted_models:
            try:
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(prompt)
                return response.text
            except: continue
        return "AI ë¶„ì„ ì‹¤íŒ¨."
    except: return "ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜."

# --- 4. Word ìƒì„± ---
def create_word_doc(report_text, keywords, title):
    doc = Document()
    doc.add_heading(title, 0)
    doc.add_paragraph(f"ìƒì„±ì¼ì: {datetime.now().strftime('%Y-%m-%d')}")
    doc.add_paragraph("-" * 50)
    for line in report_text.split('\n'):
        if line.startswith('###'): doc.add_heading(line.replace('###', '').strip(), level=3)
        elif line.startswith('##'): doc.add_heading(line.replace('##', '').strip(), level=2)
        elif line.startswith('#'): doc.add_heading(line.replace('#', '').strip(), level=1)
        elif line.startswith('**') and line.endswith('**'): 
            p = doc.add_paragraph()
            p.add_run(line.replace('**', '')).bold = True
        else: doc.add_paragraph(line)
    bio = io.BytesIO()
    doc.save(bio)
    return bio.getvalue()

# --- 5. ë©”ì¸ UI ---
st.set_page_config(page_title="Bio-Energy Tracker", layout="wide")
st.title("ğŸ”¬ ë°”ì´ì˜¤ ì—ë„ˆì§€ íŠ¸ë˜ì»¤ (ë…¼ë¬¸ & ë‰´ìŠ¤)")

if not configure_gemini():
    st.error("âŒ Google API Key ì„¤ì • í•„ìš”")

with st.sidebar:
    st.header("ğŸ” ê²€ìƒ‰ ì„¤ì •")
    default_keywords = "Biodiesel\nSustainable Aviation Fuel\nSAF\nHVO"
    keywords_input = st.text_area("ê²€ìƒ‰ì–´ (ì˜ì–´ ê¶Œì¥)", value=default_keywords, height=150)
    months = st.slider("ê²€ìƒ‰ ê¸°ê°„ (ê°œì›”)", 1, 24, 6) # ë‰´ìŠ¤ëŠ” ìµœì‹  ë™í–¥ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ê¸°ë³¸ 6ê°œì›”
    search_btn = st.button("ê²€ìƒ‰ ì‹œì‘ ğŸš€", type="primary")

# íƒ­ êµ¬ì„±
tab_global, tab_news = st.tabs(["ğŸŒ í•´ì™¸ ë…¼ë¬¸ (ê¸°ìˆ /ê³µì •)", "ğŸ“° êµ­ë‚´ì™¸ ë‰´ìŠ¤ (ì‹œì¥/ì •ì±…)"])

if search_btn:
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]
    
    # --- [íƒ­ 1] í•´ì™¸ ë…¼ë¬¸ ì²˜ë¦¬ ---
    with tab_global:
        with st.spinner("í•´ì™¸ ì „ë¬¸ DBì—ì„œ ê³µì •/ê¸°ìˆ  ë…¼ë¬¸ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            epmc_papers = get_epmc_papers(keywords, months)
            if not epmc_papers:
                st.warning("ê²€ìƒ‰ëœ í•´ì™¸ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                report_global = generate_ai_report(epmc_papers, keywords, "Global_Papers")
                docx_global = create_word_doc(report_global, keywords, "ğŸŒ¿ í•´ì™¸ ë°”ì´ì˜¤ ê³µì • ê¸°ìˆ  ë¦¬í¬íŠ¸")
                
                col1, col2 = st.columns([1, 4])
                with col1:
                    st.download_button("ğŸ“¥ ë…¼ë¬¸ ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", docx_global, "Tech_Report.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", key="btn1")
                
                st.divider()
                sub_tab1, sub_tab2 = st.tabs(["ğŸ“Š AI ê¸°ìˆ  ë¶„ì„ ë¦¬í¬íŠ¸", "ğŸ“ ì›ë¬¸ ë¦¬ìŠ¤íŠ¸"])
                with sub_tab1: st.markdown(report_global)
                with sub_tab2:
                    for i, p in enumerate(epmc_papers):
                        with st.expander(f"[{i+1}] {p['title']} ({p['date']})"):
                            st.write(p['abstract'])
                            st.markdown(f"[ì›ë¬¸ ë§í¬]({p['url']})")

    # --- [íƒ­ 2] êµ­ë‚´ì™¸ ë‰´ìŠ¤ ì²˜ë¦¬ ---
    with tab_news:
        with st.spinner("êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ êµ­ë‚´ ë° í•´ì™¸ ì‹œì¥/ì •ì±… ë™í–¥ì„ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤..."):
            news_items = get_google_news(keywords, months)
            if not news_items:
                st.warning("ê´€ë ¨ ë‰´ìŠ¤ê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            else:
                report_news = generate_ai_report(news_items, keywords, "News")
                docx_news = create_word_doc(report_news, keywords, "ğŸ“° êµ­ë‚´ì™¸ ë°”ì´ì˜¤ ì‹œì¥ ë™í–¥ ë¦¬í¬íŠ¸")
                
                col1, col2 = st.columns([1, 4])
                with col1:
                    st.download_button("ğŸ“¥ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", docx_news, "News_Report.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", key="btn2")
                
                st.divider()
                sub_tab1, sub_tab2 = st.tabs(["ğŸ“Š AI ì‹œì¥ ë¶„ì„ ë¦¬í¬íŠ¸", "ğŸ“° ë‰´ìŠ¤ ì›ë¬¸ ë¦¬ìŠ¤íŠ¸"])
                with sub_tab1: 
                    st.info("ğŸ’¡ **ì•ˆë‚´:** ì´ ë¦¬í¬íŠ¸ëŠ” ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì œëª©ì„ ê·¼ê±°ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, AIì˜ ì¶”ë¡ ì´ í¬í•¨ëœ ë¶€ë¶„ì€ ë³„ë„ë¡œ ëª…ì‹œí•˜ì˜€ìŠµë‹ˆë‹¤.")
                    st.markdown(report_news)
                with sub_tab2:
                    for i, n in enumerate(news_items):
                        with st.expander(f"[{i+1}] {n['source']} | {n['title']} ({n['date']})"):
                            st.markdown(f"**[ê¸°ì‚¬ ë°”ë¡œê°€ê¸°]({n['url']})**")
