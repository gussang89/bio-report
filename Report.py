import streamlit as st
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import google.generativeai as genai
from datetime import datetime, timedelta

# --- 1. êµ¬ê¸€ ì œë¯¸ë‚˜ì´ ì„¤ì • (ìµœì‹  ëª¨ë¸ ê°•ì œ) ---
def configure_gemini():
    try:
        if "GOOGLE_API_KEY" in st.secrets:
            genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
            return True
        else:
            st.error("Secretsì— GOOGLE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
    except Exception as e:
        st.error(f"API ì„¤ì • ì˜¤ë¥˜: {e}")
        return False

# --- 2. arXiv ë…¼ë¬¸ ê²€ìƒ‰ í•¨ìˆ˜ ---
def get_arxiv_papers(keywords, months):
    query_parts = []
    for k in keywords:
        clean_k = k.strip()
        if not clean_k: continue
        # ì œëª©(ti) ë˜ëŠ” ì´ˆë¡(abs)ì— ê²€ìƒ‰ì–´ í¬í•¨
        query_parts.append(f'(ti:{clean_k} OR abs:{clean_k})')
    
    if not query_parts:
        return []

    search_query = " OR ".join(query_parts)
    encoded_query = urllib.parse.quote(search_query)
    
    # ê²€ìƒ‰ ê°œìˆ˜ 30ê°œ
    base_url = f"http://export.arxiv.org/api/query?search_query={encoded_query}&start=0&max_results=30&sortBy=submittedDate&sortOrder=descending"
    
    try:
        with urllib.request.urlopen(base_url) as url:
            data = url.read().decode('utf-8')
            
        root = ET.fromstring(data)
        namespace = {'atom': 'http://www.w3.org/2005/Atom'}
        
        cutoff_date = datetime.now() - timedelta(days=months*30)
        filtered_papers = []
        
        for entry in root.findall('atom:entry', namespace):
            published_str = entry.find('atom:published', namespace).text
            # ë‚ ì§œ íŒŒì‹± (2024-02-12T14:00:00Z)
            published_date = datetime.strptime(published_str, "%Y-%m-%dT%H:%M:%SZ")
            
            if published_date >= cutoff_date:
                title = entry.find('atom:title', namespace).text.strip().replace('\n', ' ')
                summary = entry.find('atom:summary', namespace).text.strip().replace('\n', ' ')
                link = entry.find('atom:id', namespace).text
                
                filtered_papers.append({
                    "title": title,
                    "abstract": summary,
                    "url": link,
                    "publicationDate": published_date.strftime("%Y-%m-%d")
                })
        
        return filtered_papers

    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# --- 3. ì œë¯¸ë‚˜ì´ ë¦¬í¬íŠ¸ ì‘ì„± ---
def generate_trend_report(papers, keywords):
    if not papers: return "ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

    target_papers = papers[:15]
    combined_text = ""
    for i, p in enumerate(target_papers):
        combined_text += f"[{i+1}] {p['title']}\n"

    prompt = f"""
    ë‹¹ì‹ ì€ ë°”ì´ì˜¤ ì—ë„ˆì§€ ê³µì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    í‚¤ì›Œë“œ: {', '.join(keywords)}
    
    ì•„ë˜ {len(papers)}ê±´ì˜ ìµœì‹  ë…¼ë¬¸ ì œëª©ë“¤ì„ ë³´ê³  ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
    
    1. ğŸ” **ìš”ì•½**: "ì´ {len(papers)}ê±´ì˜ ìµœì‹  ì—°êµ¬ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤."
    2. ğŸ“ˆ **ì£¼ìš” í† í”½**: ê°€ì¥ ë§ì´ ì—°êµ¬ë˜ê³  ìˆëŠ” ë¶„ì•¼ 3ê°€ì§€ í‚¤ì›Œë“œ.
    3. ğŸ’¡ **ì¸ì‚¬ì´íŠ¸**: ì œëª©ë“¤ë¡œ ë³´ì•„ í˜„ì¬ ì—°êµ¬ì˜ íë¦„ì´ ì–´ë””ë¡œ ê°€ê³  ìˆëŠ”ì§€ í•œ ë¬¸ë‹¨ ì„¤ëª….
    
    [ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸]
    {combined_text}
    """
    
    try:
        # [í•µì‹¬ ìˆ˜ì •] ë¬´ì¡°ê±´ 'gemini-1.5-flash' ì‚¬ìš© (ê°€ì¥ ì•ˆì •ì )
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"âš ï¸ AI ë¶„ì„ ì—ëŸ¬: {e}\n(ì‚¬ì´ë“œë°”ì˜ 'ëª¨ë¸ ì§„ë‹¨'ì„ í™•ì¸í•´ì£¼ì„¸ìš”)"

# --- 4. ë©”ì¸ UI ---
st.set_page_config(page_title="Bio-Tech ArXiv Finder", layout="wide")
st.title("ğŸ”¬ ë°”ì´ì˜¤ ë…¼ë¬¸ íƒìƒ‰ê¸° (1.5 Flash Ver.)")

is_configured = configure_gemini()

with st.sidebar:
    st.header("ì„¤ì •")
    default_keywords = "Biodiesel\nBiofuel\nSAF\nBiomass\nHydrotreatment\nTransesterification"
    keywords_input = st.text_area("ê²€ìƒ‰ì–´ (ì˜ì–´)", value=default_keywords, height=200)
    months = st.slider("ê²€ìƒ‰ ê¸°ê°„ (ê°œì›”)", 1, 24, 12)
    search_btn = st.button("ê²€ìƒ‰ ì‹œì‘ ğŸ”", type="primary")
    
    st.divider()
    
    # [ì§„ë‹¨ ë„êµ¬] ë‚´ í‚¤ë¡œ ì“¸ ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ ì§„ì§œ ë­”ì§€ í™•ì¸í•˜ëŠ” ë²„íŠ¼
    with st.expander("ğŸ› ï¸ ëª¨ë¸ ì—°ê²° ìƒíƒœ ì§„ë‹¨"):
        if st.button("ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸ í™•ì¸"):
            try:
                available = []
                for m in genai.list_models():
                    if 'generateContent' in m.supported_generation_methods:
                        available.append(m.name)
                st.write(available)
            except Exception as e:
                st.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")

if search_btn and is_configured:
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]
    
    if not keywords:
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ë…¼ë¬¸ì„ ì°¾ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            papers = get_arxiv_papers(keywords, months)
            
            if not papers:
                st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë³€ê²½í•´ë³´ì„¸ìš”.")
            else:
                st.success(f"ì„±ê³µ! {len(papers)}ê±´ì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                
                col1, col2 = st.columns([1, 1])
                
                with col1:
                    st.subheader("ğŸ“Š AI íŠ¸ë Œë“œ ìš”ì•½")
                    report = generate_trend_report(papers, keywords)
                    st.markdown(report)
                
                with col2:
                    st.subheader("ğŸ“ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸")
                    for p in papers:
                        with st.expander(f"{p['title']}"):
                            st.caption(p['publicationDate'])
                            st.write(p['abstract'])
                            st.markdown(f"[ë§í¬]({p['url']})")
