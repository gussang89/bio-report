import streamlit as st
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import google.generativeai as genai
from datetime import datetime, timedelta

# --- 1. êµ¬ê¸€ ì œë¯¸ë‚˜ì´ ì„¤ì • ---
def configure_gemini():
    if "GOOGLE_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
        return True
    return False

# --- 2. arXiv ë…¼ë¬¸ ê²€ìƒ‰ í•¨ìˆ˜ ---
def get_arxiv_papers(keywords, months):
    query_parts = []
    for k in keywords:
        clean_k = k.strip()
        if not clean_k: continue
        query_parts.append(f'(ti:{clean_k} OR abs:{clean_k})')
    
    if not query_parts: return []

    search_query = " OR ".join(query_parts)
    encoded_query = urllib.parse.quote(search_query)
    base_url = f"http://export.arxiv.org/api/query?search_query={encoded_query}&start=0&max_results=30&sortBy=submittedDate&sortOrder=descending"
    
    try:
        with urllib.request.urlopen(base_url) as url:
            data = url.read().decode('utf-8')
            
        root = ET.fromstring(data)
        namespace = {'atom': 'http://www.w3.org/2005/Atom'}
        
        cutoff_date = datetime.now() - timedelta(days=months*30)
        filtered_papers = []
        
        for entry in root.findall('atom:entry', namespace):
            published_str = entry.find('atom:published', namespace).text
            published_date = datetime.strptime(published_str, "%Y-%m-%dT%H:%M:%SZ")
            
            if published_date >= cutoff_date:
                title = entry.find('atom:title', namespace).text.strip().replace('\n', ' ')
                summary = entry.find('atom:summary', namespace).text.strip().replace('\n', ' ')
                link = entry.find('atom:id', namespace).text
                filtered_papers.append({
                    "title": title, "abstract": summary, "url": link,
                    "publicationDate": published_date.strftime("%Y-%m-%d")
                })
        return filtered_papers
    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# --- 3. ì œë¯¸ë‚˜ì´ ë¦¬í¬íŠ¸ ì‘ì„± (ğŸŒŸí•µì‹¬: ë¦´ë ˆì´ í…ŒìŠ¤íŠ¸) ---
def generate_trend_report(papers, keywords):
    if not papers: return "ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

    target_papers = papers[:15]
    combined_text = ""
    for i, p in enumerate(target_papers):
        combined_text += f"[{i+1}] {p['title']}\n"

    prompt = f"""
    ë‹¹ì‹ ì€ ë°”ì´ì˜¤ ì—ë„ˆì§€ ê³µì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í‚¤ì›Œë“œ: {', '.join(keywords)}
    ì•„ë˜ {len(papers)}ê±´ì˜ ìµœì‹  ë…¼ë¬¸ ì œëª©ë“¤ì„ ë³´ê³  ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
    
    1. ğŸ” **ìš”ì•½**: "ì´ {len(papers)}ê±´ì˜ ìµœì‹  ì—°êµ¬ ë¶„ì„"
    2. ğŸ“ˆ **ì£¼ìš” í† í”½**: ê°€ì¥ ë§ì´ ì—°êµ¬ë˜ê³  ìˆëŠ” ë¶„ì•¼ 3ê°€ì§€ í‚¤ì›Œë“œ.
    3. ğŸ’¡ **ì¸ì‚¬ì´íŠ¸**: ì œëª©ë“¤ë¡œ ë³´ì•„ í˜„ì¬ ì—°êµ¬ì˜ íë¦„ì´ ì–´ë””ë¡œ ê°€ê³  ìˆëŠ”ì§€ í•œ ë¬¸ë‹¨ ì„¤ëª….
    
    [ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸]
    {combined_text}
    """
    
    # 1. ë‚´ API í‚¤ë¡œ ì“¸ ìˆ˜ ìˆëŠ” ëª¨ë“  ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    try:
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
    except Exception as e:
        return f"âš ï¸ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {e}"

    if not available_models:
        return "âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤."

    # 2. ëª©ë¡ì— ìˆëŠ” ëª¨ë¸ì„ í•˜ë‚˜ì”© ìˆœì„œëŒ€ë¡œ ì „ë¶€ ì‹œë„í•´ë³´ê¸°
    error_logs = []
    for model_name in available_models:
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            # ì„±ê³µí•˜ë©´ ê²°ê³¼ì™€ í•¨ê»˜ ì–´ë–¤ ëª¨ë¸ì´ ì„±ê³µí–ˆëŠ”ì§€ ì•Œë ¤ì£¼ê³  ë°”ë¡œ ì¢…ë£Œ!
            return f"*(âœ… `{model_name}` ëª¨ë¸ë¡œ ë¶„ì„ ì„±ê³µ!)*\n\n" + response.text
        except Exception as e:
            # ì‹¤íŒ¨í•˜ë©´ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ì–´ê°
            error_logs.append(f"- {model_name} ì‹¤íŒ¨: {e}")
            continue

    # 3. ëª¨ë“  ëª¨ë¸ì´ ë‹¤ ì‹¤íŒ¨í–ˆì„ ë•Œë§Œ ì—ëŸ¬ ì›ì¸ ì´ì¶œë ¥
    error_summary = "\n".join(error_logs)
    return f"âš ï¸ ë³´ìœ í•˜ì‹  API í‚¤ì˜ ëª¨ë“  ëª¨ë¸ì´ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\n[ì‹¤íŒ¨ ì›ì¸ë“¤]\n{error_summary}"

# --- 4. ë©”ì¸ UI ---
st.set_page_config(page_title="Bio-Tech ArXiv Finder", layout="wide")
st.title("ğŸ”¬ ë°”ì´ì˜¤ ë…¼ë¬¸ íƒìƒ‰ê¸° (Auto-Healing Ver.)")

if not configure_gemini():
    st.error("âŒ Secretsì— GOOGLE_API_KEY ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.header("ì„¤ì •")
    default_keywords = "Biodiesel\nBiofuel\nSAF\nBiomass\nHydrotreatment"
    keywords_input = st.text_area("ê²€ìƒ‰ì–´ (ì˜ì–´)", value=default_keywords, height=200)
    months = st.slider("ê²€ìƒ‰ ê¸°ê°„ (ê°œì›”)", 1, 24, 12)
    search_btn = st.button("ê²€ìƒ‰ ì‹œì‘ ğŸ”", type="primary")

if search_btn:
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]
    
    with st.spinner("ë…¼ë¬¸ì„ ì°¾ê³  ëª¨ë“  ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ ì¤‘ì…ë‹ˆë‹¤..."):
        papers = get_arxiv_papers(keywords, months)
        
        if not papers:
            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.success(f"ì„±ê³µ! {len(papers)}ê±´ì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            col1, col2 = st.columns([1, 1])
            
            with col1:
                st.subheader("ğŸ“Š AI íŠ¸ë Œë“œ ìš”ì•½")
                report = generate_trend_report(papers, keywords)
                st.markdown(report)
            
            with col2:
                st.subheader("ğŸ“ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸")
                for p in papers:
                    with st.expander(f"{p['title']}"):
                        st.caption(p['publicationDate'])
                        st.markdown(f"[ë§í¬]({p['url']})")
