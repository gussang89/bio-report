import streamlit as st
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import google.generativeai as genai
from datetime import datetime, timedelta

# --- 1. êµ¬ê¸€ ì œë¯¸ë‚˜ì´ ì„¤ì • ---
def configure_gemini():
    if "GOOGLE_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
        return True
    return False

# --- 2. arXiv ë…¼ë¬¸ ê²€ìƒ‰ í•¨ìˆ˜ ---
def get_arxiv_papers(keywords, months):
    query_parts = []
    for k in keywords:
        clean_k = k.strip()
        if not clean_k: continue
        query_parts.append(f'(ti:{clean_k} OR abs:{clean_k})')
    
    if not query_parts: return []

    search_query = " OR ".join(query_parts)
    encoded_query = urllib.parse.quote(search_query)
    # ë°ì´í„°ë¥¼ ë§ì´ ì£¼ê¸° ìœ„í•´ ê²€ìƒ‰ëŸ‰ì„ 50ê°œë¡œ ëŠ˜ë¦½ë‹ˆë‹¤.
    base_url = f"http://export.arxiv.org/api/query?search_query={encoded_query}&start=0&max_results=50&sortBy=submittedDate&sortOrder=descending"
    
    try:
        with urllib.request.urlopen(base_url) as url:
            data = url.read().decode('utf-8')
            
        root = ET.fromstring(data)
        namespace = {'atom': 'http://www.w3.org/2005/Atom'}
        
        cutoff_date = datetime.now() - timedelta(days=months*30)
        filtered_papers = []
        
        for entry in root.findall('atom:entry', namespace):
            published_str = entry.find('atom:published', namespace).text
            published_date = datetime.strptime(published_str, "%Y-%m-%dT%H:%M:%SZ")
            
            if published_date >= cutoff_date:
                title = entry.find('atom:title', namespace).text.strip().replace('\n', ' ')
                summary = entry.find('atom:summary', namespace).text.strip().replace('\n', ' ')
                link = entry.find('atom:id', namespace).text
                filtered_papers.append({
                    "title": title, "abstract": summary, "url": link,
                    "publicationDate": published_date.strftime("%Y-%m-%d")
                })
        return filtered_papers
    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# --- 3. ì œë¯¸ë‚˜ì´ ë¦¬í¬íŠ¸ ì‘ì„± (ğŸŒŸ ì‹¬ì¸µ í”„ë¡¬í”„íŠ¸ ì ìš©) ---
def generate_trend_report(papers, keywords, months):
    if not papers: return "ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

    # ë¦¬í¬íŠ¸ì˜ ì§ˆì„ ë†’ì´ê¸° ìœ„í•´ ìƒìœ„ 30ê°œ ë…¼ë¬¸ì˜ 'ì œëª©'ê³¼ 'ì´ˆë¡'ì„ ëª¨ë‘ ì œê³µí•©ë‹ˆë‹¤.
    target_papers = papers[:30]
    combined_text = ""
    for i, p in enumerate(target_papers):
        combined_text += f"[{i+1}] ì œëª©: {p['title']}\nì´ˆë¡: {p['abstract']}\n\n"

    # [í•µì‹¬] A4 2ì¥ ë¶„ëŸ‰ì„ ë½‘ì•„ë‚´ê¸° ìœ„í•œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸
    prompt = f"""
    ë‹¹ì‹ ì€ ë°”ì´ì˜¤ ì—ë„ˆì§€(Biodiesel, HVO, SAF) ê³µì • ì„¤ê³„ ë° ìµœì í™”ë¥¼ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ìˆ˜ì„ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
    ì•„ë˜ëŠ” ìµœê·¼ {months}ê°œì›”ê°„ arXivì—ì„œ ê²€ìƒ‰ëœ ë…¼ë¬¸ {len(papers)}ê±´ì˜ ì œëª©ê³¼ ì´ˆë¡ì…ë‹ˆë‹¤. (ê´€ì‹¬ í‚¤ì›Œë“œ: {', '.join(keywords)})

    ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²½ì˜ì§„ ë° í˜„ì¥ ì‹¤ë¬´ì§„ì—ê²Œ ë³´ê³ í•  **A4 2í˜ì´ì§€ ë¶„ëŸ‰(ì•½ 3000ì ì´ìƒ)ì˜ ë§¤ìš° ìƒì„¸í•˜ê³  ê¹Šì´ ìˆëŠ” 'ì‹¬ì¸µ ê¸°ìˆ  ë™í–¥ ë¦¬í¬íŠ¸'**ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‹¨ìˆœ ë‚˜ì—´ì´ ì•„ë‹Œ, ì „ë¬¸ì ì¸ ë¦¬ë·° ë…¼ë¬¸ ìˆ˜ì¤€ìœ¼ë¡œ ìœ ê¸°ì ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

    [í•„ìˆ˜ í¬í•¨ ëª©ì°¨ ë° ì‘ì„± ì§€ì¹¨]
    
    1. ğŸ“ **Executive Summary (ê±°ì‹œì  íŠ¸ë Œë“œ ì´í‰)**
       - ìˆ˜ì§‘ëœ ë…¼ë¬¸ë“¤ì„ ê´€í†µí•˜ëŠ” í•µì‹¬ ê¸°ìˆ  íŠ¸ë Œë“œëŠ” ë¬´ì—‡ì¸ì§€ 3~4ë¬¸ë‹¨ìœ¼ë¡œ ê¸¸ê³  ìƒì„¸í•˜ê²Œ ì„œìˆ .
    
    2. ğŸ”¬ **ì£¼ìš” ê¸°ìˆ  ë° ê³µì • íŠ¸ë Œë“œ ì‹¬ì¸µ ë¶„ì„**
       - ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ë¥¼ 3~4ê°œ(ì˜ˆ: ì‹ ê·œ ì´‰ë§¤ ë° ë°˜ì‘ íš¨ìœ¨, ì „ì²˜ë¦¬ ê¸°ìˆ , ëŒ€ì²´ ì›ë£Œ íƒìƒ‰ ë“±)ë¡œ ë‚˜ëˆ„ì–´ ê° ë¶„ì•¼ì˜ ì—°êµ¬ ë™í–¥ì„ ê¹Šì´ ìˆê²Œ ë¶„ì„.
    
    3. ğŸ’¡ **í˜„ì—… ê³µì • ì ìš© ë° ìµœì í™” ì¸ì‚¬ì´íŠ¸**
       - ì—°ì¤‘ë¬´íœ´(24/7)ë¡œ ê°€ë™ë˜ëŠ” ì—°ì†ì‹ ê³µì •(Continuous Process)ì˜ ì•ˆì •ì„±ì„ ë†’ì´ê±°ë‚˜, ìˆ˜ìœ¨(Yield) ê°œì„ , ìœ í‹¸ë¦¬í‹° ë¹„ìš©(ì „ë ¥, ìŠ¤íŒ€ ë“±) ì ˆê°ì— ì§ì ‘ì ìœ¼ë¡œ ì ìš©í•´ ë³¼ ìˆ˜ ìˆëŠ” ì‹¤ë¬´ì  ì•„ì´ë””ì–´ë¥¼ ë„ì¶œí•  ê²ƒ.
       - ê°€ëŠ¥í•˜ë‹¤ë©´ ëª¨ë…¸ê¸€ë¦¬ì„¸ë¼ì´ë“œ(MG) ì €ê° ë“± í’ˆì§ˆ í–¥ìƒê³¼ ì—°ê²° ì§€ì„ ê²ƒ.
    
    4. ğŸ† **í•µì‹¬ ë…¼ë¬¸ 5ì„  ì‹¬ì¸µ ë¦¬ë·°**
       - ì‚°ì—…ì  í™œìš© ê°€ì¹˜ê°€ ê°€ì¥ ë†’ì€ ë…¼ë¬¸ 5ê°œë¥¼ ì„ ì •í•˜ì—¬, ê° ë…¼ë¬¸ì˜ 1) ì—°êµ¬ ëª©ì , 2) ì ìš©ëœ í•µì‹¬ ê¸°ìˆ  ë° ìˆ˜ì¹˜ì  ì„±ê³¼, 3) í•œê³„ì  ë° ì‹œì‚¬ì ì„ ê°ê° ìƒì„¸íˆ ë¦¬ë·°í•  ê²ƒ.

    [ë…¼ë¬¸ ë°ì´í„°]
    {combined_text}
    """
    
    try:
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
    except Exception as e:
        return f"âš ï¸ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}"

    if not available_models:
        return "âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤."

    error_logs = []
    # í…ìŠ¤íŠ¸ë¥¼ ê¸¸ê²Œ ë½‘ì•„ì•¼ í•˜ë¯€ë¡œ, ë” ë˜‘ë˜‘í•œ ëª¨ë¸ì¸ 1.5-proë¥¼ ë¨¼ì € ì‹œë„í•˜ê³  flashë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
    preferred_order = ['models/gemini-1.5-pro', 'models/gemini-1.5-flash', 'models/gemini-pro']
    sorted_models = [m for m in preferred_order if m in available_models] + [m for m in available_models if m not in preferred_order]

    for model_name in sorted_models:
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            return f"*(âœ… `{model_name}` ëª¨ë¸ë¡œ ìƒì„±ëœ ì‹¬ì¸µ ë¶„ì„ ë¦¬í¬íŠ¸)*\n\n" + response.text
        except Exception as e:
            error_logs.append(f"- {model_name} ì‹¤íŒ¨: {e}")
            continue

    error_summary = "\n".join(error_logs)
    return f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨.\n\n[ì›ì¸]\n{error_summary}"

# --- 4. ë©”ì¸ UI ---
st.set_page_config(page_title="Bio-Tech Deep Report", layout="wide")
st.title("ğŸŒ¿ ë°”ì´ì˜¤ ê¸°ìˆ  ì‹¬ì¸µ íŠ¸ë Œë“œ ë¦¬í¬íŠ¸")
st.caption("AIê°€ ìµœì‹  ë…¼ë¬¸ì˜ ì´ˆë¡ì„ ëª¨ë‘ ì½ê³  A4 2í˜ì´ì§€ ë¶„ëŸ‰ì˜ ì „ë¬¸ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. (ìƒì„±ì— 30ì´ˆ~1ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")

if not configure_gemini():
    st.error("âŒ Secretsì— GOOGLE_API_KEY ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.header("ì„¤ì •")
    default_keywords = "Biodiesel\nSustainable Aviation Fuel\nTransesterification\nHVO\nBiomass"
    keywords_input = st.text_area("ê²€ìƒ‰ì–´ (ì˜ì–´)", value=default_keywords, height=200)
    months = st.slider("ê²€ìƒ‰ ê¸°ê°„ (ê°œì›”)", 1, 24, 12)
    search_btn = st.button("ì‹¬ì¸µ ë¦¬í¬íŠ¸ ìƒì„± ğŸš€", type="primary")

if search_btn:
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]
    
    with st.spinner("ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ê³ , AIê°€ ì‹¬ì¸µ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."):
        papers = get_arxiv_papers(keywords, months)
        
        if not papers:
            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.success(f"ì„±ê³µ! {len(papers)}ê±´ì˜ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
            
            # ë¦¬í¬íŠ¸ê°€ ê¸¸ì–´ì§€ë¯€ë¡œ ì»¬ëŸ¼ì„ ë‚˜ëˆ„ì§€ ì•Šê³  íƒ­ìœ¼ë¡œ í™”ë©´ì„ ë„“ê²Œ ì”ë‹ˆë‹¤.
            tab1, tab2 = st.tabs(["ğŸ“Š AI ì‹¬ì¸µ íŠ¸ë Œë“œ ë¦¬í¬íŠ¸", "ğŸ“ ë…¼ë¬¸ ì›ë¬¸ ë¦¬ìŠ¤íŠ¸"])
            
            with tab1:
                report = generate_trend_report(papers, keywords, months)
                st.markdown(report)
            
            with tab2:
                for p in papers:
                    with st.expander(f"{p['title']} ({p['publicationDate']})"):
                        st.write(p['abstract'])
                        st.markdown(f"**[ì›ë¬¸ PDF ë§í¬]({p['url']})**")
