import streamlit as st
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import google.generativeai as genai
from datetime import datetime, timedelta

# --- 1. êµ¬ê¸€ ì œë¯¸ë‚˜ì´ ì„¤ì • ---
def configure_gemini():
    try:
        if "GOOGLE_API_KEY" in st.secrets:
            api_key = st.secrets["GOOGLE_API_KEY"]
            genai.configure(api_key=api_key)
            
            available_models = []
            try:
                for m in genai.list_models():
                    if 'generateContent' in m.supported_generation_methods:
                        available_models.append(m.name)
            except:
                pass # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©

            # ìš°ì„ ìˆœìœ„: 1.5 Flash (ë¹ ë¦„) -> 1.5 Pro (ë˜‘ë˜‘í•¨) -> Pro (ë¬´ë‚œí•¨)
            preferred_models = ['models/gemini-1.5-flash', 'models/gemini-1.5-pro', 'models/gemini-pro']
            
            selected_model = 'models/gemini-pro' # ê¸°ë³¸ê°’
            for pref in preferred_models:
                if pref in available_models:
                    selected_model = pref
                    break
            return selected_model
        else:
            st.error("Secretsì— GOOGLE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
    except Exception as e:
        st.error(f"API ì„¤ì • ì˜¤ë¥˜: {e}")
        return None

MODEL_NAME = configure_gemini()

# --- 2. arXiv ë…¼ë¬¸ ê²€ìƒ‰ í•¨ìˆ˜ (ê²€ìƒ‰ëŸ‰ ëŒ€í­ ì¦ê°€ ìˆ˜ì •) ---
def get_arxiv_papers(keywords, months):
    # [ë³€ê²½ì ] ë”°ì˜´í‘œ("")ë¥¼ ì œê±°í•˜ê³  ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ë„“í˜
    # ì œëª©(ti) ë˜ëŠ” ì´ˆë¡(abs)ì— í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •
    # ì˜ˆ: (ti:biodiesel OR abs:biodiesel)
    
    query_parts = []
    for k in keywords:
        # ê³µë°±ì´ ìˆëŠ” ê²€ìƒ‰ì–´(ì˜ˆ: Bio fuel)ëŠ” ê´„í˜¸ë¡œ ë¬¶ì–´ì¤Œ
        clean_k = k.strip()
        query_parts.append(f'(ti:{clean_k} OR abs:{clean_k})')
    
    # ëª¨ë“  í‚¤ì›Œë“œë¥¼ ORë¡œ ì—°ê²° (í•˜ë‚˜ë¼ë„ ê±¸ë¦¬ë©´ ë‚˜ì˜´)
    search_query = " OR ".join(query_parts)
    encoded_query = urllib.parse.quote(search_query)
    
    # ê²€ìƒ‰ ê°œìˆ˜ë„ 30ê°œ -> 50ê°œë¡œ ëŠ˜ë¦¼
    base_url = f"http://export.arxiv.org/api/query?search_query={encoded_query}&start=0&max_results=50&sortBy=submittedDate&sortOrder=descending"
    
    try:
        with urllib.request.urlopen(base_url) as url:
            data = url.read().decode('utf-8')
            
        root = ET.fromstring(data)
        namespace = {'atom': 'http://www.w3.org/2005/Atom'}
        
        cutoff_date = datetime.now() - timedelta(days=months*30)
        filtered_papers = []
        
        for entry in root.findall('atom:entry', namespace):
            published_str = entry.find('atom:published', namespace).text
            published_date = datetime.strptime(published_str, "%Y-%m-%dT%H:%M:%SZ")
            
            if published_date >= cutoff_date:
                title = entry.find('atom:title', namespace).text.strip().replace('\n', ' ')
                summary = entry.find('atom:summary', namespace).text.strip().replace('\n', ' ')
                link = entry.find('atom:id', namespace).text
                
                filtered_papers.append({
                    "title": title,
                    "abstract": summary,
                    "url": link,
                    "publicationDate": published_date.strftime("%Y-%m-%d")
                })
        
        return filtered_papers

    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# --- 3. ì œë¯¸ë‚˜ì´ ë¦¬í¬íŠ¸ ì‘ì„± ---
def generate_trend_report(papers, keywords, months):
    if not papers: return "ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."
    if not MODEL_NAME: return "ëª¨ë¸ ì˜¤ë¥˜."

    # ë…¼ë¬¸ì´ ë§ì•„ì¡Œìœ¼ë‹ˆ ìƒìœ„ 15ê°œ ë¶„ì„
    target_papers = papers[:15]
    combined_text = ""
    for i, p in enumerate(target_papers):
        combined_text += f"[{i+1}] {p['title']}\n"

    prompt = f"""
    ë‹¹ì‹ ì€ ë°”ì´ì˜¤ ì—ë„ˆì§€ ê³µì • ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
    í‚¤ì›Œë“œ: {', '.join(keywords)}
    
    ìµœê·¼ {months}ê°œì›”ê°„ arXivì—ì„œ ê²€ìƒ‰ëœ {len(papers)}ê±´ì˜ ë…¼ë¬¸ ì œëª©ë“¤ì„ ë³´ê³  íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
    (ë‚´ìš©ì€ ì œëª©ìœ¼ë¡œ ìœ ì¶”í•˜ì„¸ìš”)
    
    1. ğŸ” **ê²€ìƒ‰ í˜„í™©**: "ì´ {len(papers)}ê±´ ë°œê²¬ë¨ (ëª¨ë¸: {MODEL_NAME})"
    2. ğŸ“ˆ **ì£¼ìš” í‚¤ì›Œë“œ**: ì œëª©ì—ì„œ ìì£¼ ë³´ì´ëŠ” ê¸°ìˆ  ìš©ì–´ 3ê°€ì§€ (ì˜ˆ: Catalytic, Pyrolysis ë“±)
    3. ğŸ’¡ **ì¸ì‚¬ì´íŠ¸**: ì—°êµ¬ íë¦„ì´ ì–´ë””ë¡œ ê°€ê³  ìˆëŠ”ì§€ í•œ ë¬¸ë‹¨ ìš”ì•½.
    
    [ë…¼ë¬¸ ì œëª© ë¦¬ìŠ¤íŠ¸]
    {combined_text}
    """
    
    try:
        model = genai.GenerativeModel(MODEL_NAME)
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"AI ë¶„ì„ ì‹¤íŒ¨: {e}"

# --- 4. ë©”ì¸ UI ---
st.set_page_config(page_title="Bio-Tech ArXiv Finder", layout="wide")
st.title("ğŸ”¬ ë°”ì´ì˜¤ ë…¼ë¬¸ íƒìƒ‰ê¸° (í™•ì¥ ê²€ìƒ‰ Ver.)")
st.caption("ì œëª©ê³¼ ì´ˆë¡ì„ ë„“ê²Œ ê²€ìƒ‰í•˜ì—¬ ë†“ì¹˜ëŠ” ë…¼ë¬¸ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.")

if MODEL_NAME:
    st.caption(f"âœ… AI ì—°ê²°ë¨: `{MODEL_NAME}`")
else:
    st.error("âŒ AI ì—°ê²° ì‹¤íŒ¨")

with st.sidebar:
    st.header("ì„¤ì •")
    # [ê°œì„ ] ê¸°ë³¸ ê²€ìƒ‰ì–´ë¥¼ ì¢€ ë” ì˜ ë‚˜ì˜¤ëŠ” ê²ƒë“¤ë¡œ ì„¸íŒ…
    default_keywords = "Biodiesel\nBiofuel\nSAF\nBiomass\nHydrotreatment\nTransesterification"
    keywords_input = st.text_area("ê²€ìƒ‰ì–´ (ì§§ì€ ë‹¨ì–´ ì¶”ì²œ)", value=default_keywords, height=200)
    months = st.slider("ê²€ìƒ‰ ê¸°ê°„ (ê°œì›”)", 1, 24, 12)
    search_btn = st.button("ê²€ìƒ‰ ì‹œì‘ ğŸ”", type="primary")

if search_btn:
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]
    
    if not keywords:
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ë” ë„“ì€ ë²”ìœ„ì—ì„œ ë…¼ë¬¸ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤..."):
            papers = get_arxiv_papers(keywords, months)
            
            if not papers:
                st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ 0ê±´ì…ë‹ˆë‹¤. 'Biodiesel' ê°™ì€ ì•„ì£¼ ë‹¨ìˆœí•œ ë‹¨ì–´ë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
            else:
                st.success(f"ì„±ê³µ! {len(papers)}ê±´ì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                
                col1, col2 = st.columns([1, 1])
                
                with col1:
                    st.subheader("ğŸ“Š AI íŠ¸ë Œë“œ ìš”ì•½")
                    report = generate_trend_report(papers, keywords, months)
                    st.markdown(report)
                
                with col2:
                    st.subheader("ğŸ“ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸")
                    for p in papers:
                        with st.expander(f"{p['title']}"):
                            st.caption(p['publicationDate'])
                            st.write(p['abstract'])
                            st.markdown(f"[ë§í¬]({p['url']})")
