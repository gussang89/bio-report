import streamlit as st
import pandas as pd
import requests
import google.generativeai as genai
from datetime import datetime, timedelta

# --- 1. êµ¬ê¸€ ì œë¯¸ë‚˜ì´ ì„¤ì • ---
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
except Exception as e:
    st.error("API í‚¤ ì„¤ì • ì—ëŸ¬: Secretsì— GOOGLE_API_KEYê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

# --- 2. ë…¼ë¬¸ ê²€ìƒ‰ í•¨ìˆ˜ (ë©€í‹° í‚¤ì›Œë“œ ì§€ì›) ---
def get_recent_papers(keywords, days=14):
    """
    ì—¬ëŸ¬ í‚¤ì›Œë“œë¥¼ ë°›ì•„ OR ì¡°ê±´ìœ¼ë¡œ í•œ ë²ˆì— ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    current_year = datetime.now().year
    year_range = f"{current_year-1}-{current_year}"
    
    # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ "keyword1 | keyword2" í˜•íƒœ(OR ê²€ìƒ‰)ë¡œ ë³€í™˜
    # Semantic ScholarëŠ” '|' ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ OR ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.
    combined_query = " | ".join(keywords)
    
    # URL ì¸ì½”ë”© ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ requests param ì‚¬ìš© ê¶Œì¥
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    params = {
        "query": combined_query,
        "year": year_range,
        "limit": 50,  # ê²€ìƒ‰ì–´ê°€ ë§ìœ¼ë¯€ë¡œ ê°€ì ¸ì˜¬ ë…¼ë¬¸ ìˆ˜ë¥¼ 50ê°œë¡œ ëŠ˜ë¦¼
        "fields": "title,abstract,url,publicationDate,venue"
    }
    
    response = requests.get(base_url, params=params)
    filtered_papers = []
    
    if response.status_code == 200:
        data = response.json().get('data', [])
        cutoff_date = datetime.now() - timedelta(days=days)
        
        for paper in data:
            pub_date_str = paper.get('publicationDate')
            if pub_date_str:
                try:
                    pub_date = datetime.strptime(pub_date_str, '%Y-%m-%d')
                    if pub_date >= cutoff_date:
                        filtered_papers.append(paper)
                except ValueError:
                    continue
    return filtered_papers

# --- 3. ì œë¯¸ë‚˜ì´ ìš”ì•½ í•¨ìˆ˜ ---
def generate_weekly_report(papers, keywords):
    if not papers:
        return "ë¶„ì„í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

    # ë…¼ë¬¸ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ìƒìœ„ 30ê°œë§Œ ë¶„ì„ (Gemini FlashëŠ” ì»¨í…ìŠ¤íŠ¸ê°€ í¼)
    target_papers = papers[:30]
    
    combined_abstracts = ""
    for i, p in enumerate(target_papers):
        combined_abstracts += f"[{i+1}] {p['title']}: {p.get('abstract', 'No abstract')} \n\n"

    keyword_str = ", ".join(keywords)

    prompt = f"""
    ë‹¹ì‹ ì€ ë°”ì´ì˜¤ ì—ë„ˆì§€ ê³µì • ìˆ˜ì„ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìê°€ ê´€ì‹¬ ìˆì–´ í•˜ëŠ” í‚¤ì›Œë“œëŠ” [{keyword_str}] ì…ë‹ˆë‹¤.
    ì•„ë˜ëŠ” ì´ì™€ ê´€ë ¨í•˜ì—¬ ìµœê·¼ 2ì£¼ê°„ ë°œí‘œëœ ë…¼ë¬¸ë“¤ì˜ ì´ˆë¡ì…ë‹ˆë‹¤.
    
    ì´ ë‚´ìš©ë“¤ì„ ì¢…í•©í•˜ì—¬ í•œêµ­ì–´ë¡œ 'ì£¼ê°„ ê¸°ìˆ  íŠ¸ë Œë“œ ë¦¬í¬íŠ¸'ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
    ë‹¨ìˆœ ë‚˜ì—´í•˜ì§€ ë§ê³ , ì„œë¡œ ì—°ê´€ëœ ê¸°ìˆ ë¼ë¦¬ ë¬¶ì–´ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.
    
    [ë³´ê³ ì„œ í˜•ì‹]
    1. ğŸ’¡ **í•µì‹¬ íŠ¸ë Œë“œ ìš”ì•½**: ì´ë²ˆ ì£¼ ê²€ìƒ‰ëœ í‚¤ì›Œë“œë“¤ê³¼ ê´€ë ¨ëœ ê¸°ìˆ  íë¦„ (3~5ì¤„)
    2. ğŸ­ **ì£¼ìš” ì¹´í…Œê³ ë¦¬ë³„ ë™í–¥**: (ì˜ˆ: SAF ì´‰ë§¤, ì „ì²˜ë¦¬ ê³µì •, ìˆ˜ìœ¨ ê°œì„  ë“±ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…)
    3. ğŸ† **ì£¼ëª©í•  ë§Œí•œ ì„±ê³¼ (Best Pick)**: í˜„ì—…ì— ë°”ë¡œ ì ìš© ê°€ëŠ¥í•˜ê±°ë‚˜ ìˆ˜ì¹˜ê°€ íšê¸°ì ì¸ ì—°êµ¬ 3ê°œ ì„ ì •
    
    [ë…¼ë¬¸ ë°ì´í„°]
    {combined_abstracts}
    """

    model = genai.GenerativeModel('gemini-1.5-flash')
    response = model.generate_content(prompt)
    return response.text

# --- 4. ë©”ì¸ UI ---
st.set_page_config(page_title="Bio-Tech Multi Report", layout="wide")
st.title("ğŸŒ¿ ì£¼ê°„ ë°”ì´ì˜¤ ê¸°ìˆ  ë©€í‹° ë¦¬í¬íŠ¸")
st.caption("ì—¬ëŸ¬ ê´€ì‹¬ì‚¬ë¥¼ í•œ ë²ˆì— ê²€ìƒ‰í•˜ê³  ì¢…í•©ì ì¸ íŠ¸ë Œë“œë¥¼ íŒŒì•…í•˜ì„¸ìš”.")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("ğŸ” ê²€ìƒ‰ ì„¤ì •")

# ê¸°ë³¸ í‚¤ì›Œë“œ ì˜ˆì‹œ
default_keywords = """Biodiesel production
Sustainable Aviation Fuel (SAF)
HVO process
Transesterification catalyst
Used Cooking Oil (UCO) pretreatment"""

# Text Areaë¡œ ë³€ê²½í•˜ì—¬ ì—¬ëŸ¬ ì¤„ ì…ë ¥ ê°€ëŠ¥í•˜ê²Œ í•¨
raw_keywords = st.sidebar.text_area(
    "ê²€ìƒ‰ì–´ ì…ë ¥ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„, ìµœëŒ€ 10ê°œ)", 
    value=default_keywords,
    height=200
)

days_filter = st.sidebar.slider("ê¸°ê°„ ì„¤ì • (ì¼)", 7, 30, 14)

if st.sidebar.button("ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±í•˜ê¸° ğŸš€"):
    # ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¦
    keyword_list = [k.strip() for k in raw_keywords.split('\n') if k.strip()]
    
    if not keyword_list:
        st.error("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        st.info(f"ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì„ ë¶„ì„í•©ë‹ˆë‹¤: {', '.join(keyword_list)}")
        
        with st.spinner('ì—¬ëŸ¬ ì£¼ì œì˜ ìµœì‹  ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ê³  íŠ¸ë Œë“œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
            recent_papers = get_recent_papers(keyword_list, days=days_filter)
            
            if not recent_papers:
                st.error(f"ìµœê·¼ {days_filter}ì¼ ë™ì•ˆ ë°œí–‰ëœ ê´€ë ¨ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì„ ëŠ˜ë ¤ë³´ì„¸ìš”.")
            else:
                st.success(f"ì´ {len(recent_papers)}ê±´ì˜ ìµœì‹  ë…¼ë¬¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                
                # ì¢…í•© ë¦¬í¬íŠ¸
                st.subheader("ğŸ“Š Gemini ì¢…í•© ê¸°ìˆ  ë¶„ì„")
                report_content = generate_weekly_report(recent_papers, keyword_list)
                st.markdown(report_content)
                
                st.divider()
                
                # ê°œë³„ ë¦¬ìŠ¤íŠ¸
                st.subheader("ğŸ“ ìˆ˜ì§‘ëœ ë…¼ë¬¸ ëª©ë¡")
                for paper in recent_papers:
                    with st.expander(f"[{paper['publicationDate']}] {paper['title']}"):
                        st.write(f"**ì €ë„:** {paper.get('venue', 'N/A')}")
                        st.write(f"**ë§í¬:** {paper['url']}")
                        st.caption(paper.get('abstract', 'ì´ˆë¡ ì—†ìŒ'))

st.sidebar.markdown("---")
st.sidebar.caption("Powered by Gemini 1.5 Flash & Semantic Scholar")
