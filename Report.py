import streamlit as st
import urllib.request
import urllib.parse
import json
import re
import google.generativeai as genai
from datetime import datetime, timedelta
import io
from docx import Document
import xml.etree.ElementTree as ET
from email.utils import parsedate_to_datetime

# --- 1. êµ¬ê¸€ ì œë¯¸ë‚˜ì´ ì„¤ì • ---
def configure_gemini():
    if "GOOGLE_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
        return True
    return False

# --- 2. ê²€ìƒ‰ í•¨ìˆ˜ë“¤ ---

# [1] í•´ì™¸ ë…¼ë¬¸ (Europe PMC)
def get_epmc_papers(keywords, months):
    query_parts = [f'({k.strip()})' for k in keywords if k.strip()]
    if not query_parts: return []
    keyword_query = " OR ".join(query_parts)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=months*30)
    date_query = f'FIRST_PDATE:[{start_date.strftime("%Y-%m-%d")} TO {end_date.strftime("%Y-%m-%d")}]'
    full_query = f"({keyword_query}) AND ({date_query})"
    encoded_query = urllib.parse.quote(full_query)
    base_url = f"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query={encoded_query}&format=json&resultType=core&pageSize=30"
    
    try:
        req = urllib.request.Request(base_url, headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req) as response:
            data = json.loads(response.read().decode('utf-8'))
        filtered = []
        for p in data.get('resultList', {}).get('result', []):
            title = p.get('title', '')
            abstract = re.sub('<[^<]+>', '', p.get('abstractText', ''))
            doi = p.get('doi')
            link = f"https://doi.org/{doi}" if doi else ""
            if title and abstract:
                filtered.append({"title": title, "abstract": abstract, "url": link, "date": p.get('firstPublicationDate', '')})
        return filtered
    except: return []

# [2] í•µì‹¬ ìˆ˜ì •: êµ­ë‚´ ë‰´ìŠ¤ (ë„¤ì´ë²„ ë‰´ìŠ¤ RSS ì ìš© - API í‚¤ ë¶ˆí•„ìš”)
def get_domestic_news(keywords, months):
    news_list = []
    headers = {'User-Agent': 'Mozilla/5.0'}
    cutoff_date = datetime.now() - timedelta(days=months*30)
    
    for k in keywords:
        clean_k = k.strip()
        if not clean_k: continue
        
        encoded_query = urllib.parse.quote(clean_k)
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ RSS URL
        url = f"https://newssearch.naver.com/search.naver?where=rss&query={encoded_query}"
        
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req) as response:
                root = ET.fromstring(response.read())
            
            for item in root.findall('./channel/item'):
                title = item.find('title').text
                link = item.find('link').text
                pubDate = item.find('pubDate').text
                description = item.find('description').text
                
                # ë‚ ì§œ íŒŒì‹± ë° ê¸°ê°„ í•„í„°ë§
                try: 
                    dt = parsedate_to_datetime(pubDate)
                    # ì„¤ì •í•œ ê¸°ê°„ ì´ì „ì˜ ë‰´ìŠ¤ëŠ” ë²„ë¦¼
                    if dt.replace(tzinfo=None) < cutoff_date.replace(tzinfo=None):
                        continue
                    date_str = dt.strftime("%Y-%m-%d")
                except: 
                    date_str = pubDate
                
                # ì œëª©ê³¼ ì´ˆë¡ì˜ ë¶ˆí•„ìš”í•œ HTML íƒœê·¸ ê¹”ë”í•˜ê²Œ ì œê±°
                clean_title = re.sub('<[^<]+>', '', title)
                clean_abstract = re.sub('<[^<]+>', '', description) if description else "ìƒì„¸ ë‚´ìš©ì€ ë§í¬ ì°¸ê³ "
                
                news_list.append({"title": clean_title, "abstract": clean_abstract[:300], "url": link, "date": date_str})
        except Exception as e:
            st.warning(f"'{clean_k}' ë„¤ì´ë²„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
            
    unique_news = {n['url']: n for n in news_list}.values()
    return sorted(unique_news, key=lambda x: x['date'], reverse=True)

# [3] í•µì‹¬ ìˆ˜ì •: í•´ì™¸ ë‰´ìŠ¤ (êµ¬ê¸€ ë‰´ìŠ¤ ë²„ê·¸ ìš°íšŒ)
def get_overseas_news(keywords, months):
    news_list = []
    headers = {'User-Agent': 'Mozilla/5.0'}
    cutoff_date = datetime.now() - timedelta(days=months*30)
    
    for k in keywords:
        clean_k = k.strip()
        if not clean_k: continue
        
        encoded_query = urllib.parse.quote(clean_k)
        # êµ¬ê¸€ ì„œë²„ ë²„ê·¸ë¥¼ ì¼ìœ¼í‚¤ëŠ” when ì˜µì…˜ì„ ë¹¼ê³ , íŒŒì´ì¬ì—ì„œ ë‚ ì§œë¥¼ ê±¸ëŸ¬ëƒ…ë‹ˆë‹¤.
        url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
        
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req) as response:
                root = ET.fromstring(response.read())
            
            for item in root.findall('./channel/item'):
                title = item.find('title').text
                link = item.find('link').text
                pubDate = item.find('pubDate').text
                
                try: 
                    dt = parsedate_to_datetime(pubDate)
                    if dt.replace(tzinfo=None) < cutoff_date.replace(tzinfo=None):
                        continue
                    date_str = dt.strftime("%Y-%m-%d")
                except: 
                    date_str = pubDate
                    
                news_list.append({"title": title, "abstract": "ìƒì„¸ ë‚´ìš©ì€ ì›ë¬¸ ì°¸ì¡°", "url": link, "date": date_str})
        except Exception as e:
            continue
            
    unique_news = {n['url']: n for n in news_list}.values()
    return sorted(unique_news, key=lambda x: x['date'], reverse=True)

# --- 3. AI ë¦¬í¬íŠ¸ ìƒì„± ---
def generate_ai_report(items, keywords, section_type):
    if not items: return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    data_text = ""
    for i, item in enumerate(items[:30]):
        data_text += f"[{i+1}] ì œëª©: {item['title']} (ì¼ì: {item['date']})\nì´ˆë¡: {item['abstract'][:200]}\n\n"

    if section_type == "Paper":
        role = "ê¸€ë¡œë²Œ ë°”ì´ì˜¤ ê³µì • ì—°êµ¬ì›"
        focus = "ìµœì‹  ê³µì • ê¸°ìˆ , ìˆ˜ìœ¨ ê°œì„ , ì´‰ë§¤ ë™í–¥ ë¶„ì„"
    elif section_type == "Domestic_News":
        role = "í•œêµ­ ë°”ì´ì˜¤ ì—ë„ˆì§€ ì‹œì¥ ì• ë„ë¦¬ìŠ¤íŠ¸"
        focus = "êµ­ë‚´ ì •ì±… ë³€í™”, ì •ìœ /ë°”ì´ì˜¤ ê¸°ì—…ì˜ ë™í–¥, ê·œì œ íë¦„"
    else:
        role = "ê¸€ë¡œë²Œ ë°”ì´ì˜¤ ì—ë„ˆì§€ ì‹œì¥ ì• ë„ë¦¬ìŠ¤íŠ¸"
        focus = "í•´ì™¸ ì„ ì§„êµ­ì˜ ìƒìš©í™” ë™í–¥, ì£¼ìš” ê·œì œ, ê¸€ë¡œë²Œ ê¸°ì—… íˆ¬ì ë™í–¥"

    prompt = f"""
    ë‹¹ì‹ ì€ {role}ì…ë‹ˆë‹¤. í‚¤ì›Œë“œ: {', '.join(keywords)}
    
    ì•„ë˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ **'ì‹¬ì¸µ ë³´ê³ ì„œ'**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    
    [í•µì‹¬ ì¤€ìˆ˜ ì‚¬í•­: ì •í™•ì„± ë° ê·¼ê±° í‘œê¸°]
    1. ëª¨ë“  ì„œìˆ ì€ ë°˜ë“œì‹œ ì œê³µëœ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ í•´ì•¼ í•˜ë©°, ë¬¸ì¥ì´ë‚˜ ë‹¨ë½ ëì— ë°˜ë“œì‹œ ì¶œì²˜ ì£¼ì„(ì˜ˆ: [1], [3])ì„ ë‹¬ì•„ì£¼ì„¸ìš”.
    2. ë°ì´í„°ë§Œìœ¼ë¡œ ëª…í™•íˆ ì•Œ ìˆ˜ ì—†ì–´ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•˜ê±°ë‚˜ ì• ë§¤í•œ ë¶€ë¶„ì— ëŒ€í•´ì„œëŠ”, ë°˜ë“œì‹œ **"â€» ì¶”ë¡ : ë³¸ ë‚´ìš©ì€ ëª…ì‹œëœ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ë¡ ëœ ê²ƒìœ¼ë¡œ ì •í™•ì„±ì— í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."**ë¼ê³  ì„œìˆ í•˜ì„¸ìš”. ì—†ëŠ” ë‚´ìš©ì„ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

    [ì‘ì„± í¬ì¸íŠ¸]
    1. ğŸ“Š **í•µì‹¬ íŠ¸ë Œë“œ ìš”ì•½**: {focus}
    2. ğŸ’¡ **ì„¸ë¶€ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸**: ì£¼ìš” ì´ìŠˆ ë° í˜„ì—… ì ìš©/ëŒ€ì‘ ì‹œì‚¬ì 
    3. ğŸ“Œ **ì£¼ìš” ì›ë¬¸ ë¦¬ë·°**: í•µì‹¬ ë°ì´í„° ë²ˆí˜¸ ê¸°ì¬í•˜ì—¬ ë¦¬ë·°

    [ìˆ˜ì§‘ëœ ë°ì´í„°]
    {data_text}
    """
    
    try:
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        preferred_order = ['models/gemini-1.5-pro', 'models/gemini-1.5-flash', 'models/gemini-pro']
        sorted_models = [m for m in preferred_order if m in available_models] + [m for m in available_models if m not in preferred_order]

        for model_name in sorted_models:
            try:
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(prompt)
                return response.text
            except: continue
        return "AI ë¶„ì„ ì‹¤íŒ¨."
    except: return "ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜."

# --- 4. Word ìƒì„± ---
def create_word_doc(report_text, keywords, title):
    doc = Document()
    doc.add_heading(title, 0)
    doc.add_paragraph(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords)}")
    doc.add_paragraph("-" * 50)
    for line in report_text.split('\n'):
        if line.startswith('###'): doc.add_heading(line.replace('###', '').strip(), level=3)
        elif line.startswith('##'): doc.add_heading(line.replace('##', '').strip(), level=2)
        elif line.startswith('#'): doc.add_heading(line.replace('#', '').strip(), level=1)
        elif line.startswith('**') and line.endswith('**'): 
            p = doc.add_paragraph()
            p.add_run(line.replace('**', '')).bold = True
        else: doc.add_paragraph(line)
    bio = io.BytesIO()
    doc.save(bio)
    return bio.getvalue()

# --- 5. ë©”ì¸ UI ---
st.set_page_config(page_title="Bio-Energy Tracker", layout="wide")
st.title("ğŸ”¬ ë°”ì´ì˜¤ ì—ë„ˆì§€ í†µí•© íŠ¸ë˜ì»¤ (ë„¤ì´ë²„ ë‰´ìŠ¤ íƒ‘ì¬)")
st.caption("ê° íƒ­ì—ì„œ ì›í•˜ëŠ” ì£¼ì œì˜ ê²€ìƒ‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ ê°œë³„ì ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.")

if not configure_gemini():
    st.error("âŒ Google API Key ì„¤ì • í•„ìš”")

with st.sidebar:
    st.header("ğŸ” ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ì–´ ì„¤ì •")
    
    st.subheader("1. í•´ì™¸ ë…¼ë¬¸ (ì˜ì–´)")
    paper_keywords = st.text_area("ê³µì •/ê¸°ìˆ  í‚¤ì›Œë“œ", value="Biodiesel production\nTransesterification catalyst", height=100)
    
    st.subheader("2. êµ­ë‚´ ë‰´ìŠ¤ (í•œê¸€)")
    domestic_keywords = st.text_area("êµ­ë‚´ ì‹œì¥/ì •ì±… í‚¤ì›Œë“œ", value="ë°”ì´ì˜¤ë””ì ¤\nì§€ì†ê°€ëŠ¥í•­ê³µìœ \nì—ì“°ì˜¤ì¼ ë°”ì´ì˜¤\nHDí˜„ëŒ€ì˜¤ì¼ë±…í¬ ë°”ì´ì˜¤", height=100)
    
    st.subheader("3. í•´ì™¸ ë‰´ìŠ¤ (ì˜ì–´)")
    overseas_keywords = st.text_area("í•´ì™¸ ì‹œì¥/ì •ì±… í‚¤ì›Œë“œ", value="Sustainable Aviation Fuel\nHVO market\nNeste biofuel", height=100)
    
    st.divider()
    months = st.slider("ê²€ìƒ‰ ê¸°ê°„ (ê°œì›”)", 1, 24, 6)

tab_paper, tab_domestic, tab_overseas = st.tabs(["ğŸŒ ë…¼ë¬¸ ë¶„ì„ (í•´ì™¸ ê¸°ìˆ )", "ğŸ‡°ğŸ‡· êµ­ë‚´ ë‰´ìŠ¤ ë¶„ì„ (ë„¤ì´ë²„)", "ğŸŒ í•´ì™¸ ë‰´ìŠ¤ ë¶„ì„"])

with tab_paper:
    st.markdown("### ğŸŒ í•´ì™¸ ë°”ì´ì˜¤ ê³µì • ê¸°ìˆ  íƒìƒ‰")
    if st.button("í•´ì™¸ ë…¼ë¬¸ ê²€ìƒ‰ ë° ë¶„ì„ ğŸš€", key="btn_run_paper"):
        k_paper = [k.strip() for k in paper_keywords.split('\n') if k.strip()]
        if not k_paper: st.warning("ê²€ìƒ‰ì–´ë¥¼ ì‚¬ì´ë“œë°”ì— ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("í•´ì™¸ ë…¼ë¬¸ì„ ìˆ˜ì§‘ ë° ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                papers = get_epmc_papers(k_paper, months)
                if not papers: st.warning("ê²€ìƒ‰ëœ í•´ì™¸ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.success(f"ì„±ê³µ! {len(papers)}ê±´ì˜ ë…¼ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.")
                    report_paper = generate_ai_report(papers, k_paper, "Paper")
                    docx_paper = create_word_doc(report_paper, k_paper, "ğŸŒ ë°”ì´ì˜¤ ë…¼ë¬¸/ê¸°ìˆ  ë¶„ì„ ë¦¬í¬íŠ¸")
                    st.download_button("ğŸ“¥ ë…¼ë¬¸ ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", docx_paper, "Paper_Report.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", key="btn_dl_paper")
                    st.divider()
                    st.markdown(report_paper)
                    with st.expander("ğŸ“ ìˆ˜ì§‘ëœ ë…¼ë¬¸ ì›ë¬¸ ë¦¬ìŠ¤íŠ¸ ë³´ê¸°"):
                        for i, p in enumerate(papers):
                            st.write(f"**[{i+1}] {p['title']}** ({p['date']})  [ë§í¬]({p['url']})")

with tab_domestic:
    st.markdown("### ğŸ‡°ğŸ‡· êµ­ë‚´ ë°”ì´ì˜¤ ì‹œì¥ ë° ì •ì±… íƒìƒ‰")
    if st.button("êµ­ë‚´ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ğŸš€", key="btn_run_domestic"):
        k_domestic = [k.strip() for k in domestic_keywords.split('\n') if k.strip()]
        if not k_domestic: st.warning("ê²€ìƒ‰ì–´ë¥¼ ì‚¬ì´ë“œë°”ì— ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ë„¤ì´ë²„ì—ì„œ êµ­ë‚´ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘ ë° ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                d_news = get_domestic_news(k_domestic, months)
                if not d_news: st.warning("ê²€ìƒ‰ëœ êµ­ë‚´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë°”ê¿”ë³´ì„¸ìš”.")
                else:
                    st.success(f"ì„±ê³µ! {len(d_news)}ê±´ì˜ ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.")
                    report_domestic = generate_ai_report(d_news, k_domestic, "Domestic_News")
                    docx_domestic = create_word_doc(report_domestic, k_domestic, "ğŸ‡°ğŸ‡· êµ­ë‚´ ë°”ì´ì˜¤ ì‹œì¥/ì •ì±… ë¦¬í¬íŠ¸")
                    st.download_button("ğŸ“¥ êµ­ë‚´ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", docx_domestic, "Domestic_News_Report.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", key="btn_dl_domestic")
                    st.divider()
                    st.markdown(report_domestic)
                    with st.expander("ğŸ“ ìˆ˜ì§‘ëœ êµ­ë‚´ ë‰´ìŠ¤ ì›ë¬¸ ë¦¬ìŠ¤íŠ¸ ë³´ê¸°"):
                        for i, n in enumerate(d_news):
                            st.write(f"**[{i+1}] {n['title']}** ({n['date']})  [ë§í¬]({n['url']})")

with tab_overseas:
    st.markdown("### ğŸŒ í•´ì™¸ ë°”ì´ì˜¤ ì‹œì¥ ë° ì •ì±… íƒìƒ‰")
    if st.button("í•´ì™¸ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ğŸš€", key="btn_run_overseas"):
        k_overseas = [k.strip() for k in overseas_keywords.split('\n') if k.strip()]
        if not k_overseas: st.warning("ê²€ìƒ‰ì–´ë¥¼ ì‚¬ì´ë“œë°”ì— ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("í•´ì™¸ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘ ë° ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                o_news = get_overseas_news(k_overseas, months)
                if not o_news: st.warning("ê²€ìƒ‰ëœ í•´ì™¸ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë°”ê¿”ë³´ì„¸ìš”.")
                else:
                    st.success(f"ì„±ê³µ! {len(o_news)}ê±´ì˜ ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.")
                    report_overseas = generate_ai_report(o_news, k_overseas, "Overseas_News")
                    docx_overseas = create_word_doc(report_overseas, k_overseas, "ğŸŒ í•´ì™¸ ë°”ì´ì˜¤ ì‹œì¥/ì •ì±… ë¦¬í¬íŠ¸")
                    st.download_button("ğŸ“¥ í•´ì™¸ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", docx_overseas, "Overseas_News_Report.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", key="btn_dl_overseas")
                    st.divider()
                    st.markdown(report_overseas)
                    with st.expander("ğŸ“ ìˆ˜ì§‘ëœ í•´ì™¸ ë‰´ìŠ¤ ì›ë¬¸ ë¦¬ìŠ¤íŠ¸ ë³´ê¸°"):
                        for i, n in enumerate(o_news):
                            st.write(f"**[{i+1}] {n['title']}** ({n['date']})  [ë§í¬]({n['url']})")
