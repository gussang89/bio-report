import streamlit as st
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import google.generativeai as genai
from datetime import datetime, timedelta

# --- 1. êµ¬ê¸€ ì œë¯¸ë‚˜ì´ ì„¤ì • ---
try:
    if "GOOGLE_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    else:
        st.error("Secretsì— GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
except Exception as e:
    st.error(f"API í‚¤ ì„¤ì • ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

# --- 2. arXiv ë…¼ë¬¸ ê²€ìƒ‰ í•¨ìˆ˜ (ì°¨ë‹¨ ì—†ëŠ” API) ---
def get_arxiv_papers(keywords, months):
    # arXiv APIëŠ” 'all:í‚¤ì›Œë“œ' í˜•íƒœë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    # ì˜ˆ: (all:biodiesel OR all:SAF)
    query_parts = [f'all:"{k}"' for k in keywords]
    search_query = " OR ".join(query_parts)
    
    # URL ì¸ì½”ë”© (íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬)
    encoded_query = urllib.parse.quote(search_query)
    
    # ìµœì‹ ìˆœ ì •ë ¬ (submittedDate), 30ê°œë§Œ ê°€ì ¸ì˜´
    base_url = f"http://export.arxiv.org/api/query?search_query={encoded_query}&start=0&max_results=30&sortBy=submittedDate&sortOrder=descending"
    
    try:
        with urllib.request.urlopen(base_url) as url:
            data = url.read().decode('utf-8')
            
        # XML íŒŒì‹± (arXivëŠ” XMLë¡œ ë°ì´í„°ë¥¼ ì¤ë‹ˆë‹¤)
        root = ET.fromstring(data)
        namespace = {'atom': 'http://www.w3.org/2005/Atom'}
        
        cutoff_date = datetime.now() - timedelta(days=months*30)
        filtered_papers = []
        
        for entry in root.findall('atom:entry', namespace):
            published_str = entry.find('atom:published', namespace).text
            # ë‚ ì§œ í˜•ì‹: 2024-02-12T14:00:00Z
            published_date = datetime.strptime(published_str, "%Y-%m-%dT%H:%M:%SZ")
            
            if published_date >= cutoff_date:
                title = entry.find('atom:title', namespace).text.strip().replace('\n', ' ')
                summary = entry.find('atom:summary', namespace).text.strip().replace('\n', ' ')
                link = entry.find('atom:id', namespace).text
                
                filtered_papers.append({
                    "title": title,
                    "abstract": summary,
                    "url": link,
                    "publicationDate": published_date.strftime("%Y-%m-%d")
                })
        
        return filtered_papers

    except Exception as e:
        st.error(f"arXiv ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# --- 3. ì œë¯¸ë‚˜ì´ ë¦¬í¬íŠ¸ ì‘ì„± ---
def generate_trend_report(papers, keywords, months):
    if not papers:
        return "ë¶„ì„í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

    # ìƒìœ„ 15ê°œë§Œ ë¶„ì„
    target_papers = papers[:15]
    
    combined_text = ""
    for i, p in enumerate(target_papers):
        combined_text += f"[{i+1}] ë‚ ì§œ: {p['publicationDate']} / ì œëª©: {p['title']} / ì´ˆë¡: {p['abstract'][:300]}...\n\n"

    prompt = f"""
    ë‹¹ì‹ ì€ ë°”ì´ì˜¤ ì—ë„ˆì§€ ê³µì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì‚¬ìš©ì í‚¤ì›Œë“œ: {', '.join(keywords)}
    
    ì•„ë˜ëŠ” 'arXiv(ì•„ì¹´ì´ë¸Œ)'ì—ì„œ ê²€ìƒ‰ëœ ìµœê·¼ {months}ê°œì›”ê°„ì˜ ë…¼ë¬¸ ì´ˆë¡ì…ë‹ˆë‹¤.
    ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ 'ê¸°ìˆ  ë™í–¥ ë¸Œë¦¬í•‘'ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
    
    [ì‘ì„± í¬ì¸íŠ¸]
    1. ğŸ” **ê²€ìƒ‰ ê²°ê³¼**: "arXivì—ì„œ ì´ {len(papers)}ê±´ì˜ ìµœì‹  ì—°êµ¬ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤."
    2. ğŸ’¡ **í•µì‹¬ ìš”ì•½**: ê²€ìƒ‰ëœ ì—°êµ¬ë“¤ì˜ ê¸°ìˆ ì  íŠ¹ì§• ìš”ì•½.
    3. ğŸš€ **ì£¼ìš” ë…¼ë¬¸ 3ê°€ì§€**: ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë…¼ë¬¸ 3ê°œë¥¼ ë½‘ì•„ ê°„ë‹¨íˆ ì„¤ëª….
    
    [ë…¼ë¬¸ ë°ì´í„°]
    {combined_text}
    """
    
    model = genai.GenerativeModel('gemini-1.5-flash')
    response = model.generate_content(prompt)
    return response.text

# --- 4. ë©”ì¸ UI ---
st.set_page_config(page_title="ArXiv Bio-Tech Report", layout="wide")
st.title("ğŸ”¬ ìµœì‹  ë°”ì´ì˜¤ ë…¼ë¬¸ íƒìƒ‰ê¸° (arXiv ë²„ì „)")
st.caption("ì•ˆì •ì ì¸ arXiv APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëŠê¹€ ì—†ì´ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ì„¤ì •")
    
    default_keywords = "Biodiesel\nBiofuel\nSustainable Aviation Fuel"
    
    keywords_input = st.text_area("ê²€ìƒ‰ì–´ (ì˜ì–´, ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)", value=default_keywords, height=150)
    months = st.slider("ê²€ìƒ‰ ê¸°ê°„ (ê°œì›”)", 1, 24, 12) # ê¸°ë³¸ 12ê°œì›” (arXivëŠ” ë°ì´í„°ê°€ ì•„ì£¼ ë§ì§„ ì•Šìœ¼ë¯€ë¡œ ê¸¸ê²Œ ì¡ìŒ)
    
    search_btn = st.button("ê²€ìƒ‰ ì‹œì‘ ğŸ”", type="primary")

if search_btn:
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]
    
    if not keywords:
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner(f"arXivì—ì„œ ìµœê·¼ {months}ê°œì›”ê°„ì˜ ë…¼ë¬¸ì„ ì°¾ëŠ” ì¤‘..."):
            papers = get_arxiv_papers(keywords, months)
            
            if not papers:
                st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì„ ëŠ˜ë¦¬ê±°ë‚˜ ê²€ìƒ‰ì–´ë¥¼ ë” ë„“ê²Œ ì¡ì•„ë³´ì„¸ìš”.")
            else:
                tab1, tab2 = st.tabs(["ğŸ“Š AI ë¶„ì„ ë¦¬í¬íŠ¸", "ğŸ“ ë…¼ë¬¸ ì›ë¬¸ ë¦¬ìŠ¤íŠ¸"])
                
                with tab1:
                    st.success(f"ì„±ê³µ! {len(papers)}ê±´ì˜ ë…¼ë¬¸ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                    report = generate_trend_report(papers, keywords, months)
                    st.markdown(report)
                    
                with tab2:
                    for p in papers:
                        with st.expander(f"[{p['publicationDate']}] {p['title']}"):
                            st.markdown(f"**[ë…¼ë¬¸ ë°”ë¡œê°€ê¸° (PDF)]({p['url']})**")
                            st.write(p['abstract'])
