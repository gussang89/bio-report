import streamlit as st
import pandas as pd
import requests
from openai import OpenAI
from datetime import datetime, timedelta

# --- 1. ì„¤ì • ë° API í‚¤ ---
# ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” st.secretsë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.
client = OpenAI(api_key="YOUR_OPENAI_API_KEY")

# --- 2. ë‚ ì§œ ê³„ì‚° ë° ë…¼ë¬¸ ê²€ìƒ‰ í•¨ìˆ˜ ---
def get_recent_papers(query, days=14):
    """
    ìµœê·¼ Nì¼ ì´ë‚´ì˜ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤.
    Semantic ScholarëŠ” ì •í™•í•œ ì¼ì ê²€ìƒ‰ì´ ì–´ë µìœ¼ë¯€ë¡œ, ìµœê·¼ ì—°ë„ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ Pythonìœ¼ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    # ë„‰ë„‰í•˜ê²Œ ìµœê·¼ 2ë…„ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´ (API íš¨ìœ¨ì„±ì„ ìœ„í•´)
    current_year = datetime.now().year
    year_range = f"{current_year-1}-{current_year}"
    
    url = f"https://api.semanticscholar.org/graph/v1/paper/search?query={query}&year={year_range}&limit=30&fields=title,abstract,url,publicationDate,venue"
    
    response = requests.get(url)
    filtered_papers = []
    
    if response.status_code == 200:
        data = response.json().get('data', [])
        cutoff_date = datetime.now() - timedelta(days=days)
        
        for paper in data:
            pub_date_str = paper.get('publicationDate')
            if pub_date_str:
                try:
                    pub_date = datetime.strptime(pub_date_str, '%Y-%m-%d')
                    # ë‚ ì§œ ë¹„êµ: ì„¤ì •í•œ ê¸°ê°„(2ì£¼) ì´ë‚´ì¸ì§€ í™•ì¸
                    if pub_date >= cutoff_date:
                        filtered_papers.append(paper)
                except ValueError:
                    continue # ë‚ ì§œ í˜•ì‹ì´ ì•ˆ ë§ìœ¼ë©´ íŒ¨ìŠ¤
                    
    return filtered_papers

# --- 3. AI ìš”ì•½ í•¨ìˆ˜ (ê°œë³„ ìš”ì•½ + ì¢…í•© ë¦¬í¬íŠ¸) ---
def generate_weekly_report(papers):
    """
    ìˆ˜ì§‘ëœ ë…¼ë¬¸ë“¤ì˜ ì´ˆë¡ì„ ëª¨ì•„ì„œ 'ì£¼ê°„ ê¸°ìˆ  íŠ¸ë Œë“œ'ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
    """
    if not papers:
        return "ë¶„ì„í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

    # ì´ˆë¡ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹¨ (í† í° ì œí•œ ê³ ë ¤í•˜ì—¬ ì•ë¶€ë¶„ë§Œ ì¼ë¶€ ë°œì·Œ ê°€ëŠ¥)
    combined_abstracts = ""
    for i, p in enumerate(papers):
        combined_abstracts += f"[{i+1}] {p['title']}: {p.get('abstract', 'No abstract')} \n\n"

    prompt = f"""
    ë‹¹ì‹ ì€ ë°”ì´ì˜¤ ì—°ë£Œ ê³µì • ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì•„ë˜ëŠ” ìµœê·¼ 2ì£¼ê°„ ë°œí‘œëœ ë°”ì´ì˜¤ë””ì ¤/SAF ê´€ë ¨ ë…¼ë¬¸ë“¤ì˜ ì´ˆë¡ ëª¨ìŒì…ë‹ˆë‹¤.
    
    ì´ ë‚´ìš©ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ 'ì£¼ê°„ ê¸°ìˆ  ë™í–¥ ë¦¬í¬íŠ¸'ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
    ë‹¤ìŒ ì„¸ ê°€ì§€ í•­ëª©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í•œêµ­ì–´ë¡œ ì •ë¦¬í•˜ì„¸ìš”:
    
    1. **í•µì‹¬ íŠ¸ë Œë“œ**: ì´ë²ˆ ì£¼ ì—°êµ¬ë“¤ì´ ê³µí†µì ìœ¼ë¡œ ì£¼ëª©í•˜ëŠ” ê¸°ìˆ ì´ë‚˜ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€? (ì˜ˆ: íŠ¹ì • ì´‰ë§¤, ì „ì²˜ë¦¬ ë°©ì‹ ë“±)
    2. **ì£¼ëª©í•  ë§Œí•œ ì„±ê³¼**: ìˆ˜ìœ¨ í–¥ìƒì´ë‚˜ ë¹„ìš© ì ˆê° ë“± êµ¬ì²´ì ì¸ ìˆ«ìê°€ ì–¸ê¸‰ëœ íšê¸°ì ì¸ ì—°êµ¬ê°€ ìˆë‹¤ë©´ 1~2ê°œ ê¼½ì•„ì£¼ì„¸ìš”.
    3. **í˜„ì¥ ì ìš© ê°€ëŠ¥ì„±**: ì‹¤ì œ ê³µì¥ì— ì ìš©í•´ë³¼ ë§Œí•œ ì•„ì´ë””ì–´ê°€ ìˆëŠ”ê°€?

    [ë…¼ë¬¸ ë°ì´í„°]
    {combined_abstracts}
    """

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# --- 4. ë©”ì¸ UI ---
st.set_page_config(page_title="Weekly Bio-Tech Report", layout="wide")

st.title("ğŸ“… ì£¼ê°„ ë°”ì´ì˜¤ ê¸°ìˆ  íŠ¸ë Œë“œ ë¦¬í¬íŠ¸")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("ì„¤ì •")
search_query = st.sidebar.text_input("ê²€ìƒ‰ì–´", value="Biodiesel production optimization")
days_filter = st.sidebar.slider("ê¸°ê°„ ì„¤ì • (ì¼)", 7, 30, 14) # ê¸°ë³¸ 14ì¼(2ì£¼)

if st.sidebar.button("ë¦¬í¬íŠ¸ ìƒì„±í•˜ê¸°"):
    with st.spinner(f'ìµœê·¼ {days_filter}ì¼ê°„ì˜ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
        # 1. ë…¼ë¬¸ ìˆ˜ì§‘
        recent_papers = get_recent_papers(search_query, days=days_filter)
        
        if not recent_papers:
            st.error(f"ìµœê·¼ {days_filter}ì¼ ë™ì•ˆ ë°œí–‰ëœ ê´€ë ¨ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì„ ëŠ˜ë¦¬ê±°ë‚˜ ê²€ìƒ‰ì–´ë¥¼ ë³€ê²½í•´ë³´ì„¸ìš”.")
        else:
            st.success(f"ì´ {len(recent_papers)}ê±´ì˜ ìµœì‹  ë…¼ë¬¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
            
            # 2. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± (ê°€ì¥ ìƒë‹¨ì— ë°°ì¹˜)
            st.subheader("ğŸ“Š AI ê¸°ìˆ  ë¶„ì„ ë¦¬í¬íŠ¸")
            report_content = generate_weekly_report(recent_papers)
            st.info(report_content)
            
            st.divider()
            
            # 3. ê°œë³„ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
            st.subheader("ğŸ“ ê°œë³„ ë…¼ë¬¸ ëª©ë¡")
            for paper in recent_papers:
                with st.expander(f"[{paper['publicationDate']}] {paper['title']}"):
                    st.write(f"**ì €ë„:** {paper.get('venue', 'N/A')}")
                    st.write(f"**ë§í¬:** {paper['url']}")
                    st.caption(paper.get('abstract', 'ì´ˆë¡ ì—†ìŒ'))

st.sidebar.markdown("---")
st.sidebar.caption("Data source: Semantic Scholar API")