import streamlit as st
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import google.generativeai as genai
from datetime import datetime, timedelta

# --- 1. êµ¬ê¸€ ì œë¯¸ë‚˜ì´ ì„¤ì • ---
try:
    if "GOOGLE_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    else:
        st.error("Secretsì— GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
except Exception as e:
    st.error(f"API í‚¤ ì„¤ì • ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

# --- 2. arXiv ë…¼ë¬¸ ê²€ìƒ‰ í•¨ìˆ˜ (ì•ˆì •ì ì¸ API) ---
def get_arxiv_papers(keywords, months):
    # arXiv API ì¿¼ë¦¬ ìƒì„±
    query_parts = [f'all:"{k}"' for k in keywords]
    search_query = " OR ".join(query_parts)
    encoded_query = urllib.parse.quote(search_query)
    
    # ìµœì‹ ìˆœ ì •ë ¬, 20ê°œ ê°€ì ¸ì˜¤ê¸°
    base_url = f"http://export.arxiv.org/api/query?search_query={encoded_query}&start=0&max_results=20&sortBy=submittedDate&sortOrder=descending"
    
    try:
        with urllib.request.urlopen(base_url) as url:
            data = url.read().decode('utf-8')
            
        root = ET.fromstring(data)
        namespace = {'atom': 'http://www.w3.org/2005/Atom'}
        
        cutoff_date = datetime.now() - timedelta(days=months*30)
        filtered_papers = []
        
        for entry in root.findall('atom:entry', namespace):
            published_str = entry.find('atom:published', namespace).text
            # ë‚ ì§œ íŒŒì‹± (2024-02-12T14:00:00Z)
            published_date = datetime.strptime(published_str, "%Y-%m-%dT%H:%M:%SZ")
            
            if published_date >= cutoff_date:
                title = entry.find('atom:title', namespace).text.strip().replace('\n', ' ')
                summary = entry.find('atom:summary', namespace).text.strip().replace('\n', ' ')
                link_id = entry.find('atom:id', namespace).text
                
                filtered_papers.append({
                    "title": title,
                    "abstract": summary,
                    "url": link_id,
                    "publicationDate": published_date.strftime("%Y-%m-%d")
                })
        
        return filtered_papers

    except Exception as e:
        st.error(f"arXiv ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# --- 3. ì œë¯¸ë‚˜ì´ ë¦¬í¬íŠ¸ ì‘ì„± (ëª¨ë¸ ë³€ê²½ë¨) ---
def generate_trend_report(papers, keywords, months):
    if not papers:
        return "ë¶„ì„í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."

    # Gemini ProëŠ” ì…ë ¥ ì œí•œì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒìœ„ 10ê°œë§Œ ë¶„ì„
    target_papers = papers[:10]
    
    combined_text = ""
    for i, p in enumerate(target_papers):
        combined_text += f"[{i+1}] Title: {p['title']}\nAbstract: {p['abstract'][:200]}...\n\n"

    prompt = f"""
    ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ë°”ì´ì˜¤ ì—ë„ˆì§€ ì—°êµ¬ì›ì…ë‹ˆë‹¤.
    ì‚¬ìš©ì í‚¤ì›Œë“œ: {', '.join(keywords)}
    
    ì•„ë˜ëŠ” arXivì—ì„œ ê²€ìƒ‰ëœ ìµœê·¼ {months}ê°œì›”ê°„ì˜ ë…¼ë¬¸ ìš”ì•½ë³¸ì…ë‹ˆë‹¤.
    ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ 'ìµœì‹  ê¸°ìˆ  ë™í–¥ ë¸Œë¦¬í•‘'ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
    
    [ì‘ì„± í˜•ì‹]
    1. ğŸ” **ê²€ìƒ‰ ê²°ê³¼**: "ì´ {len(papers)}ê±´ì˜ ë…¼ë¬¸ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤."
    2. ğŸ’¡ **ê¸°ìˆ  íŠ¸ë Œë“œ ìš”ì•½**: ê²€ìƒ‰ëœ ì—°êµ¬ë“¤ì˜ ì£¼ìš” ì£¼ì œì™€ íë¦„ì„ 3ì¤„ë¡œ ìš”ì•½.
    3. ğŸš€ **ì£¼ëª©í•  ë…¼ë¬¸**: ê°€ì¥ í¥ë¯¸ë¡œìš´ ë…¼ë¬¸ 2~3ê°œë¥¼ ê³¨ë¼ ì œëª©ê³¼ ë‚´ìš©ì„ ê°„ë‹¨íˆ ì†Œê°œ.
    
    [ë…¼ë¬¸ ë°ì´í„°]
    {combined_text}
    """
    
    try:
        # [ìˆ˜ì •] ê°€ì¥ í˜¸í™˜ì„±ì´ ì¢‹ì€ 'gemini-pro' ëª¨ë¸ ì‚¬ìš©
        model = genai.GenerativeModel('gemini-pro')
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# --- 4. ë©”ì¸ UI ---
st.set_page_config(page_title="ArXiv Bio-Tech Report", layout="wide")
st.title("ğŸ”¬ ìµœì‹  ë°”ì´ì˜¤ ë…¼ë¬¸ íƒìƒ‰ê¸° (Stable Ver.)")
st.caption("arXiv ë°ì´í„°ë² ì´ìŠ¤ì™€ Google Gemini Proë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.header("ì„¤ì •")
    default_keywords = "Biodiesel\nBiofuel\nSustainable Aviation Fuel"
    keywords_input = st.text_area("ê²€ìƒ‰ì–´ (ì˜ì–´)", value=default_keywords, height=150)
    months = st.slider("ê²€ìƒ‰ ê¸°ê°„ (ê°œì›”)", 1, 24, 12)
    search_btn = st.button("ê²€ìƒ‰ ì‹œì‘ ğŸ”", type="primary")

if search_btn:
    keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]
    
    if not keywords:
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            papers = get_arxiv_papers(keywords, months)
            
            if not papers:
                st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì„ ëŠ˜ë¦¬ê±°ë‚˜ ê²€ìƒ‰ì–´ë¥¼ ë³€ê²½í•´ë³´ì„¸ìš”.")
            else:
                tab1, tab2 = st.tabs(["ğŸ“Š AI ë¶„ì„ ë¦¬í¬íŠ¸", "ğŸ“ ë…¼ë¬¸ ì›ë¬¸ ë¦¬ìŠ¤íŠ¸"])
                
                with tab1:
                    st.success("ë¶„ì„ ì™„ë£Œ!")
                    report = generate_trend_report(papers, keywords, months)
                    st.markdown(report)
                    
                with tab2:
                    for p in papers:
                        with st.expander(f"[{p['publicationDate']}] {p['title']}"):
                            st.markdown(f"**[ë…¼ë¬¸ ë°”ë¡œê°€ê¸°]({p['url']})**")
                            st.write(p['abstract'])
